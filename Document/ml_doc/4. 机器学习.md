# 4. 机器学习

## 4.1 监督学习

### ==分类==

#### 1. k近邻
![](img\欧式距离1.png)
![](img\欧式距离2.png)

```python
sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)

- n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数
```



##### [交叉验证、网格搜索（模型选择与调优）](../ref/机器学习（算法篇）/K-近邻算法/section10.html)

* 交叉验证目的：**为了让被评估的模型更加准确可信**

* 网格搜索把超参数的值,通过字典的形式传递进去,然后进行选择最优值



```python
sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)

- 对估计器的指定参数值进行详尽搜索
- estimator：估计器对象
- param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}
- cv：指定几折交叉验证
- 
- fit：输入训练数据
- score：准确率
- 结果分析：
  - bestscore__:在交叉验证中验证的最好结果*
  - bestestimator：最好的参数模型
  - cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果


```



#### 2. 支持向量机

#### 3. 朴素贝叶斯

#### 4. 决策树

#### 5. 随机森林



---

#### 6. 逻辑回归

**API：**

```python
sklearn.linear_model.LogisticRegression(solver='liblinear', penalty=‘l2’, C = 1.0)

#solver可选参数:{'liblinear', 'sag', 'saga','newton-cg', 'lbfgs'}，

#默认: 'liblinear'；用于优化问题的算法。
#对于小数据集来说，“liblinear”是个不错的选择，而“sag”和'saga'对于大型数据集会更快。

#对于多类问题，只有'newton-cg'， 'sag'， 'saga'和'lbfgs'可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。

#penalty：正则化的种类

#C：正则化力度
```





**逻辑回归的原理：**

- 输入：
  - 线性回归的输出
  - ![](img\逻辑回归输入.png)
- 激活函数
  - sigmoid函数
    - ![](img\sigmoid函数.png)
  - 把整体的值映射到[0,1]
    - ![](img\sigmoid图像.png)
  - 再设置一个阈值(默认0.5)，进行分类判断



---



##### 损失以及优化

**损失**：

逻辑回归的损失，称之为**对数似然损失**

![](img\log图像.png)



* y: 真实值
* *hθ*(*x*): 预测值



分情况讨论，对应的损失函数值（==判断对于正例的概率，否则为反例==）：

- **当y=1时，我们希望hθ(x)值越大越好；**
- **当y=0时，我们希望hθ(x)值越小越好**



**优化**：

同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，**提升原本属于1类别的概率，降低原本是0类别的概率。**

---



#### 7. 神经网络



### ==回归==

#### 1. 线型回归
线性回归(Linear regression)是利用**回归方程(函数)**对**一个或多个自变量(特征值)和因变量(目标值)之间**关系进行建模的一种分析方式。

- 特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归
- 线性回归的分类
  - 线性关系
  - 非线性关系

![](img\线性回归公式.png)

* w: 权重
* b: 偏置

---



##### 损失函数

(损失：真实结果与预测的结果之间存在的误差)

![](img\线性回归损失函数.png)

- yi为第i个训练样本的真实值
- h(xi)为第i个训练样本特征值组合预测函数
- 又称最小二乘法



#####　损失优化

| 梯度下降             | 正规方程                        |
| -------------------- | ------------------------------- |
| 需要选择学习率       | 不需要                          |
| 需要迭代求解         | 一次运算得出                    |
| 特征数量较大可以使用 | 需要计算方程，时间复杂度高O(n3) |

---



###### 正规方程

![](img\正规方程.png)

![](img\正规方程求解图示.png)



**API:**

```python
sklearn.linear_model.LinearRegression(fit_intercept=True)
#通过正规方程优化

#参数:
#fit_intercept：是否计算偏置

#属性:
#LinearRegression.coef_：回归系数
#LinearRegression.intercept_：偏置
```







###### 梯度下降

![](img\梯度下降公式.png)

* α: 梯度下降算法中被称作为**学习率**或者**步长**，意味着我们可以通过α来控制每一步走的距离。

 

![](img\线性回归优化动态图.gif)





**API:**

```python
sklearn.linear_model.SGDRegressor(loss="squared_loss", fit_intercept=True, learning_rate ='invscaling', eta0=0.01)
#SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。

#参数：
#loss:损失类型
#loss=”squared_loss”: 普通最小二乘法
#fit_intercept：是否计算偏置
#learning_rate : string, optional

#学习率:
#'constant': eta = eta0
#'optimal': eta = 1.0 / (alpha * (t + t0)) [default]
#'invscaling': eta = eta0 / pow(t, power_t)
#power_t=0.25:存在父类当中
#对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。

#属性：
#SGDRegressor.coef_：回归系数
#SGDRegressor.intercept_：偏置
```



---



#### 2. 岭回归

`(解决线型回归过拟合问题)`

```python
sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)
#具有l2正则化的线性回归

#alpha:正则化力度，也叫 λ
#λ取值：0~1 1~10
- 正则化力度越大，权重系数会越小
- 正则化力度越小，权重系数会越大

#solver:会根据数据自动选择优化方法
#sag:如果数据集、特征都比较大，选择该随机梯度下降优化
#normalize:数据是否进行标准化
#normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据

#Ridge.coef_:回归权重
#Ridge.intercept_:回归偏置

#----------------------------------------

sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)
#具有l2正则化的线性回归，可以进行交叉验证

#coef_:回归系数
```





## 4.2 无监督学习



### ==聚类==

#### 1. k-means

```python
sklearn.cluster.KMeans(n_clusters=8)
# 参数:
# n_clusters:开始的聚类中心数量
# 整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数。

# 方法:
# estimator.fit(x)
# estimator.predict(x)
# estimator.fit_predict(x)
# 计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)
```

